/**
 * Tests for Unicode NFC normalization in the lexer
 */

import { describe, expect, it } from "vitest";

import { Lexer } from "./lexer.js";

describe("Lexer - Unicode NFC Normalization for Identifiers", () => {
    it("should normalize composed vs decomposed Ã© (cafÃ©)", () => {
        // Composed: cafÃ© using U+00E9 (single character Ã©)
        const composed = "caf\u00E9";
        // Decomposed: cafÃ© using U+0065 U+0301 (e + combining acute accent)
        const decomposed = "cafe\u0301";

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        // Both should produce the same normalized identifier
        expect(tokens1[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "cafÃ©",
        });
        expect(tokens2[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "cafÃ©",
        });

        // Verify they're identical after normalization
        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
    });

    it("should normalize composed vs decomposed Ã± (maÃ±ana)", () => {
        // Composed: Ã± using U+00F1
        const composed = "ma\u00F1ana";
        // Decomposed: Ã± using U+006E U+0303 (n + combining tilde)
        const decomposed = "man\u0303ana";

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
        expect(tokens1[0]!.value).toBe("maÃ±ana");
    });

    it("should normalize composed vs decomposed Ã¼ (schÃ¶n)", () => {
        // Composed: Ã¼ using U+00FC
        const composed = "sch\u00F6n";
        // Decomposed: Ã¼ using U+006F U+0308 (o + combining diaeresis)
        const decomposed = "scho\u0308n";

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
        expect(tokens1[0]!.value).toBe("schÃ¶n");
    });

    it("should normalize multiple accented characters", () => {
        // naÃ¯ve: i with diaeresis
        // Composed: Ã¯ using U+00EF
        const composed = "na\u00EFve";
        // Decomposed: Ã¯ using U+0069 U+0308 (i + combining diaeresis)
        const decomposed = "nai\u0308ve";

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
        expect(tokens1[0]!.value).toBe("naÃ¯ve");
    });

    it("should normalize identifiers in expressions", () => {
        // let cafÃ© = 42
        const composed = "let caf\u00E9 = 42";
        const decomposed = "let cafe\u0301 = 42";

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        // Find the identifier token (skip 'let' keyword)
        const id1 = tokens1.find((t) => t.type === "IDENTIFIER");
        const id2 = tokens2.find((t) => t.type === "IDENTIFIER");

        expect(id1).toBeDefined();
        expect(id2).toBeDefined();
        expect(id1!.value).toBe(id2!.value);
        expect(id1!.value).toBe("cafÃ©");
    });

    it("should handle ASCII identifiers without normalization overhead", () => {
        // ASCII identifiers should pass through unchanged
        const lexer = new Lexer("helloWorld", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "helloWorld",
        });
    });
});

describe("Lexer - Unicode NFC Normalization for Strings", () => {
    it("should normalize composed vs decomposed Ã© in string literals", () => {
        // Composed
        const composed = '"caf\u00E9"';
        // Decomposed
        const decomposed = '"cafe\u0301"';

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]).toMatchObject({
            type: "STRING_LITERAL",
            value: "cafÃ©",
        });
        expect(tokens2[0]).toMatchObject({
            type: "STRING_LITERAL",
            value: "cafÃ©",
        });

        // Verify they're identical after normalization
        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
    });

    it("should normalize strings with multiple accented characters", () => {
        // "ZÃ¼rich naÃ¯ve cafÃ©"
        const composed = '"Z\u00FCrich na\u00EFve caf\u00E9"';
        const decomposed = '"Zu\u0308rich nai\u0308ve cafe\u0301"';

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
        expect(tokens1[0]!.value).toBe("ZÃ¼rich naÃ¯ve cafÃ©");
    });

    it("should normalize multi-line strings", () => {
        // Multi-line string with accented characters
        const composed = '"""caf\u00E9\nmaÃ±ana"""';
        const decomposed = '"""cafe\u0301\nman\u0303ana"""';

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
        expect(tokens1[0]!.value).toBe("cafÃ©\nmaÃ±ana");
    });

    it("should handle strings with escape sequences and Unicode", () => {
        // String with both escape sequences and Unicode
        const composed = '"Hello\\ncaf\u00E9"';
        const decomposed = '"Hello\\ncafe\u0301"';

        const lexer1 = new Lexer(composed, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
        expect(tokens1[0]!.value).toBe("Hello\ncafÃ©");
    });

    it("should normalize Unicode escape sequences in strings", () => {
        // String using Unicode escape sequences
        const escapeSeq = '"caf\\u00E9"'; // \u00E9 is composed Ã©
        const decomposed = '"cafe\u0301"'; // Decomposed Ã©

        const lexer1 = new Lexer(escapeSeq, "test.vf");
        const lexer2 = new Lexer(decomposed, "test.vf");

        const tokens1 = lexer1.tokenize();
        const tokens2 = lexer2.tokenize();

        // After escape processing and normalization, should be identical
        expect(tokens1[0]!.value).toBe("cafÃ©");
        expect(tokens2[0]!.value).toBe("cafÃ©");
        expect(tokens1[0]!.value).toBe(tokens2[0]!.value);
    });

    it("should handle ASCII strings without normalization overhead", () => {
        const lexer = new Lexer('"Hello World"', "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "STRING_LITERAL",
            value: "Hello World",
        });
    });
});

describe("Lexer - Unicode Normalization Edge Cases", () => {
    it("should normalize combining characters in various positions", () => {
        // Test multiple combining characters
        const lexer = new Lexer("e\u0301\u0302", "test.vf"); // e + acute + circumflex
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
        });
        // After NFC normalization, the value should be consistent
        expect(typeof tokens[0]!.value).toBe("string");
    });

    it("should preserve emoji and other Unicode characters", () => {
        // Emoji should pass through unchanged (they're already in NFC form)
        const lexer = new Lexer('"Hello ðŸ‘‹ World ðŸŒ"', "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "STRING_LITERAL",
            value: "Hello ðŸ‘‹ World ðŸŒ",
        });
    });

    it("should handle Greek letters with diacritics", () => {
        // Greek letter with diacritic
        // Composed: Î¬ (alpha with tonos) U+03AC
        const composed = "\u03AC"; // Greek small letter alpha with tonos
        // Note: Greek letters typically don't have a separate decomposed form in common use
        // but we test that normalization doesn't break them

        const lexer = new Lexer(composed, "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "Î¬",
        });
    });
});
