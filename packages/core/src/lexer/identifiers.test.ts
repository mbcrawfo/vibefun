/**
 * Tests for identifier, keyword, and boolean literal parsing
 *
 * Phase 3: Simple Tokens - Identifier and keyword recognition
 */

import { describe, expect, it } from "vitest";

import { Lexer } from "./lexer.js";

describe("Lexer - Identifiers", () => {
    describe("basic identifiers", () => {
        it("should tokenize single letter identifier", () => {
            const lexer = new Lexer("x", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens).toHaveLength(2); // IDENTIFIER + EOF
            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "x",
            });
        });

        it("should tokenize multi-letter identifier", () => {
            const lexer = new Lexer("hello", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "hello",
            });
        });

        it("should tokenize camelCase identifier", () => {
            const lexer = new Lexer("getUserName", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "getUserName",
            });
        });

        it("should tokenize PascalCase identifier", () => {
            const lexer = new Lexer("PersonRecord", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "PersonRecord",
            });
        });

        it("should tokenize identifier with underscore prefix", () => {
            const lexer = new Lexer("_private", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "_private",
            });
        });

        it("should tokenize identifier with underscores", () => {
            const lexer = new Lexer("some_long_name", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "some_long_name",
            });
        });

        it("should tokenize identifier with digits", () => {
            const lexer = new Lexer("var123", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "var123",
            });
        });

        it("should tokenize identifier ending with digit", () => {
            const lexer = new Lexer("x2", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "x2",
            });
        });
    });

    describe("unicode identifiers", () => {
        it("should tokenize identifier with accented characters", () => {
            const lexer = new Lexer("cafÃ©", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "cafÃ©",
            });
        });

        it("should tokenize identifier with Greek letters", () => {
            const lexer = new Lexer("Î±Î²Î³", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "Î±Î²Î³",
            });
        });

        it("should tokenize identifier with Cyrillic letters", () => {
            const lexer = new Lexer("Ð¿Ñ€Ð¸Ð²ÐµÑ‚", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "Ð¿Ñ€Ð¸Ð²ÐµÑ‚",
            });
        });

        it("should tokenize identifier with Chinese characters", () => {
            const lexer = new Lexer("å˜é‡", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "å˜é‡",
            });
        });

        it("should tokenize identifier with Japanese characters", () => {
            const lexer = new Lexer("å¤‰æ•°", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "å¤‰æ•°",
            });
        });

        it("should tokenize mixed unicode identifier", () => {
            const lexer = new Lexer("Î±Value123", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "Î±Value123",
            });
        });
    });

    describe("emoji identifiers", () => {
        it("should tokenize single emoji as identifier", () => {
            const lexer = new Lexer("ðŸš€", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens).toHaveLength(2); // IDENTIFIER + EOF
            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸš€",
            });
        });

        it("should tokenize rocket emoji identifier", () => {
            const lexer = new Lexer("ðŸŒŸ", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸŒŸ",
            });
        });

        it("should tokenize boom emoji identifier", () => {
            const lexer = new Lexer("ðŸ’¥", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸ’¥",
            });
        });

        it("should tokenize fire emoji identifier", () => {
            const lexer = new Lexer("ðŸ”¥", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸ”¥",
            });
        });

        it("should tokenize multiple emoji as single identifier", () => {
            const lexer = new Lexer("ðŸš€ðŸŒŸ", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens).toHaveLength(2); // IDENTIFIER + EOF
            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸš€ðŸŒŸ",
            });
        });

        it("should tokenize emoji mixed with letters", () => {
            const lexer = new Lexer("rocketðŸš€", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "rocketðŸš€",
            });
        });

        it("should tokenize emoji mixed with Greek letters", () => {
            const lexer = new Lexer("Ï€ðŸš€", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "Ï€ðŸš€",
            });
        });

        it("should tokenize emoji starting identifier with numbers", () => {
            const lexer = new Lexer("ðŸš€123", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸš€123",
            });
        });

        it("should tokenize complex emoji with skin tone modifier", () => {
            const lexer = new Lexer("ðŸ‘¨â€ðŸ’»", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸ‘¨â€ðŸ’»",
            });
        });

        it("should tokenize thumbs up with skin tone", () => {
            const lexer = new Lexer("ðŸ‘ðŸ½", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸ‘ðŸ½",
            });
        });

        it("should handle emoji in let binding", () => {
            const lexer = new Lexer("let ðŸš€ = 42", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]?.type).toBe("KEYWORD");
            expect(tokens[1]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸš€",
            });
            expect(tokens[2]?.type).toBe("OP_EQUALS");
            expect(tokens[3]).toMatchObject({
                type: "INT_LITERAL",
                value: 42,
            });
        });

        it("should handle emoji in function definition", () => {
            const lexer = new Lexer("let ðŸŒŸ = () => 123", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[1]).toMatchObject({
                type: "IDENTIFIER",
                value: "ðŸŒŸ",
            });
        });

        it("should separate emoji identifiers with whitespace", () => {
            const lexer = new Lexer("ðŸš€ ðŸŒŸ ðŸ’¥", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens).toHaveLength(4); // 3 identifiers + EOF
            expect(tokens[0]?.value).toBe("ðŸš€");
            expect(tokens[1]?.value).toBe("ðŸŒŸ");
            expect(tokens[2]?.value).toBe("ðŸ’¥");
        });

        it("should tokenize emoji with underscore", () => {
            const lexer = new Lexer("_ðŸš€", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "_ðŸš€",
            });
        });

        it("should tokenize emoji between underscores", () => {
            const lexer = new Lexer("_ðŸš€_", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "_ðŸš€_",
            });
        });
    });

    describe("multiple identifiers", () => {
        it("should tokenize space-separated identifiers", () => {
            const lexer = new Lexer("foo bar baz", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens).toHaveLength(4); // 3 identifiers + EOF
            expect(tokens[0]?.value).toBe("foo");
            expect(tokens[1]?.value).toBe("bar");
            expect(tokens[2]?.value).toBe("baz");
        });

        it("should tokenize newline-separated identifiers", () => {
            const lexer = new Lexer("foo\nbar\nbaz", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens).toHaveLength(6); // 3 identifiers + 2 newlines + 1 EOF
            expect(tokens[0]?.type).toBe("IDENTIFIER");
            expect(tokens[1]?.type).toBe("NEWLINE");
            expect(tokens[2]?.type).toBe("IDENTIFIER");
            expect(tokens[3]?.type).toBe("NEWLINE");
            expect(tokens[4]?.type).toBe("IDENTIFIER");
        });

        it("should tokenize identifiers with punctuation", () => {
            const lexer = new Lexer("(foo,bar)", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]?.type).toBe("LPAREN");
            expect(tokens[1]?.type).toBe("IDENTIFIER");
            expect(tokens[2]?.type).toBe("COMMA");
            expect(tokens[3]?.type).toBe("IDENTIFIER");
            expect(tokens[4]?.type).toBe("RPAREN");
        });
    });

    describe("identifier boundaries", () => {
        it("should stop identifier at whitespace", () => {
            const lexer = new Lexer("hello world", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "IDENTIFIER",
                value: "hello",
            });
            expect(tokens[1]).toMatchObject({
                type: "IDENTIFIER",
                value: "world",
            });
        });

        it("should stop identifier at punctuation", () => {
            const lexer = new Lexer("foo(bar)", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]?.value).toBe("foo");
            expect(tokens[1]?.type).toBe("LPAREN");
            expect(tokens[2]?.value).toBe("bar");
        });

        it("should stop identifier at operator", () => {
            const lexer = new Lexer("x+y", "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]?.value).toBe("x");
            expect(tokens[1]?.type).toBe("OP_PLUS");
            expect(tokens[2]?.value).toBe("y");
        });
    });
});

describe("Lexer - Keywords", () => {
    const keywords = [
        "let",
        "mut",
        "type",
        "if",
        "then",
        "else",
        "match",
        "when",
        "rec",
        "and",
        "import",
        "export",
        "external",
        "unsafe",
        "from",
        "as",
        "try",
        "catch",
    ];

    keywords.forEach((keyword) => {
        it(`should recognize '${keyword}' as keyword`, () => {
            const lexer = new Lexer(keyword, "test.vf");
            const tokens = lexer.tokenize();

            expect(tokens[0]).toMatchObject({
                type: "KEYWORD",
                value: keyword,
                keyword: keyword,
            });
        });
    });

    it("should tokenize all 18 keywords", () => {
        expect(keywords).toHaveLength(18);
    });

    it("should tokenize keyword in expression", () => {
        const lexer = new Lexer("let x", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "KEYWORD",
            value: "let",
            keyword: "let",
        });
        expect(tokens[1]).toMatchObject({
            type: "IDENTIFIER",
            value: "x",
        });
    });

    it("should distinguish keyword from similar identifier", () => {
        const lexer = new Lexer("letter", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "letter",
        });
    });

    it("should handle keywords with punctuation", () => {
        const lexer = new Lexer("if(x)", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]?.type).toBe("KEYWORD");
        expect(tokens[0]?.value).toBe("if");
        expect(tokens[1]?.type).toBe("LPAREN");
    });

    it("should be case-sensitive for keywords", () => {
        const lexer = new Lexer("Let LET", "test.vf");
        const tokens = lexer.tokenize();

        // Both should be identifiers, not keywords
        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "Let",
        });
        expect(tokens[1]).toMatchObject({
            type: "IDENTIFIER",
            value: "LET",
        });
    });
});

describe("Lexer - Boolean Literals", () => {
    it("should tokenize true as boolean literal", () => {
        const lexer = new Lexer("true", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "BOOL_LITERAL",
            value: true,
        });
    });

    it("should tokenize false as boolean literal", () => {
        const lexer = new Lexer("false", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "BOOL_LITERAL",
            value: false,
        });
    });

    it("should distinguish true from True", () => {
        const lexer = new Lexer("True", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "True",
        });
    });

    it("should tokenize multiple boolean literals", () => {
        const lexer = new Lexer("true false true", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]?.value).toBe(true);
        expect(tokens[1]?.value).toBe(false);
        expect(tokens[2]?.value).toBe(true);
    });

    it("should handle booleans in expressions", () => {
        const lexer = new Lexer("let x = true", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]?.type).toBe("KEYWORD");
        expect(tokens[1]?.type).toBe("IDENTIFIER");
        expect(tokens[2]?.type).toBe("OP_EQUALS");
        expect(tokens[3]).toMatchObject({
            type: "BOOL_LITERAL",
            value: true,
        });
    });
});

describe("Lexer - Identifier Edge Cases", () => {
    it("should handle single underscore as identifier", () => {
        const lexer = new Lexer("_", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "_",
        });
    });

    it("should handle multiple underscores", () => {
        const lexer = new Lexer("___", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: "___",
        });
    });

    it("should handle very long identifier", () => {
        const longName = "a".repeat(1000);
        const lexer = new Lexer(longName, "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]).toMatchObject({
            type: "IDENTIFIER",
            value: longName,
        });
    });

    it("should track location correctly for identifier", () => {
        const lexer = new Lexer("  hello", "test.vf");
        const tokens = lexer.tokenize();

        expect(tokens[0]?.loc).toMatchObject({
            line: 1,
            column: 3, // After two spaces
        });
    });
});
